{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9942ce76",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05cb45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# df must already exist with column: text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c5698",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c9a89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df[\"text\"].head())\n",
    "print(df[\"text\"].apply(len).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e007de0",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da99f15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text_basic\"] = df[\"text\"].apply(basic_clean)\n",
    "df[[\"text\",\"clean_text_basic\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849420af",
   "metadata": {},
   "source": [
    "# Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f77de",
   "metadata": {},
   "source": [
    "# Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f7a06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def advanced_clean(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df[\"clean_text_advanced\"] = df[\"clean_text_basic\"].apply(advanced_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bd9fc",
   "metadata": {},
   "source": [
    "# Task4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509baa1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "df[\"text_no_stopwords\"] = df[\"clean_text_advanced\"].apply(\n",
    "    lambda x: \" \".join([w for w in x.split() if w not in stop_words])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce99d26",
   "metadata": {},
   "source": [
    "# Part3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f5292",
   "metadata": {},
   "source": [
    "# Task5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7990ff1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "slang = {\"u\":\"you\", \"gr8\":\"great\"}\n",
    "\n",
    "def normalize(text):\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    return \" \".join([slang.get(w, w) for w in text.split()])\n",
    "\n",
    "df[\"normalized_text\"] = df[\"text_no_stopwords\"].apply(normalize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435bb03f",
   "metadata": {},
   "source": [
    "# Task6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618587f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df[\"word_tokens\"] = df[\"normalized_text\"].apply(word_tokenize)\n",
    "df[\"sentence_tokens\"] = df[\"text\"].apply(sent_tokenize)\n",
    "\n",
    "df[[\"word_tokens\",\"sentence_tokens\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b791a0d",
   "metadata": {},
   "source": [
    "# Part4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb334a0",
   "metadata": {},
   "source": [
    "# Task7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2594a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "df[\"stemmed\"] = df[\"word_tokens\"].apply(lambda x: [stemmer.stem(w) for w in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd3913",
   "metadata": {},
   "source": [
    "# Task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616c7fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"lemmatized\"] = df[\"word_tokens\"].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf55e7b",
   "metadata": {},
   "source": [
    "# Part5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5794de0",
   "metadata": {},
   "source": [
    "# Task9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc481adc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def nlp_preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|\\S+@\\S+|<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"final_clean_text\"] = df[\"text\"].apply(nlp_preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05851f",
   "metadata": {},
   "source": [
    "# Task10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db318818",
   "metadata": {},
   "source": [
    "1. Difference between basic & advanced cleaning\n",
    "\n",
    "Basic cleaning removes case, punctuation, numbers, and spaces.\n",
    "\n",
    "Advanced cleaning removes noise like URLs, emails, emojis, HTML, stopwords and normalizes words → much cleaner and model-ready.\n",
    "\n",
    "2. Why lemmatization is preferred over stemming\n",
    "\n",
    "Stemming cuts words blindly (e.g., running → runn).\n",
    "\n",
    "Lemmatization returns real dictionary words (e.g., running → run), preserving meaning.\n",
    "\n",
    "3. Importance of preprocessing in NLP\n",
    "\n",
    "Reduces noise\n",
    "\n",
    "Improves model accuracy\n",
    "\n",
    "Reduces vocabulary size\n",
    "\n",
    "Makes text consistent and meaningful"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
