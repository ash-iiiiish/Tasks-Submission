{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193bc367",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992db23d",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cddd4",
   "metadata": {},
   "source": [
    "1. What are word embeddings?\n",
    "\n",
    "Word embeddings are dense numerical vectors that represent words in a continuous vector space where semantically similar words are close together.\n",
    "Example:\n",
    "king → [0.12, -0.45, ...]\n",
    "queen → [0.11, -0.44, ...]\n",
    "\n",
    "2. Why one-hot encoding and BoW fail?\n",
    "Method\tProblem\n",
    "One-Hot\tHuge sparse vectors, no meaning in distances\n",
    "Bag of Words\tIgnores word order & context, treats all words as independent\n",
    "\n",
    "They cannot capture relationships like:\n",
    "king - man + woman ≈ queen.\n",
    "\n",
    "3. How word embeddings solve this?\n",
    "\n",
    "Words are learned from context\n",
    "\n",
    "Similar words get similar vectors\n",
    "\n",
    "Captures semantic & syntactic meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73a07c",
   "metadata": {},
   "source": [
    "# Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07be44",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec591d4",
   "metadata": {},
   "source": [
    "What is Word2Vec?\n",
    "\n",
    "Word2Vec is a shallow neural network model that learns word embeddings by predicting words from context.\n",
    "\n",
    "Predicting from context\n",
    "\n",
    "If the sentence is:\n",
    "\n",
    "I love machine learning\n",
    "\n",
    "Word2Vec learns that machine and learning are related because they appear together.\n",
    "\n",
    "Definitions\n",
    "\n",
    "Vocabulary: All unique words in the dataset\n",
    "\n",
    "Context window: Number of words around a target\n",
    "\n",
    "Embedding dimension: Length of each word vector (e.g., 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0fc6a",
   "metadata": {},
   "source": [
    "# Task3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7972f",
   "metadata": {},
   "source": [
    "Model\tDescription\n",
    "CBOW\tPredicts target word from context words\n",
    "Skip-Gram\tPredicts context words from target\n",
    "When to use:\n",
    "\n",
    "CBOW: Faster, good for large datasets\n",
    "\n",
    "Skip-Gram: Better for rare words, smaller data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11efe3b",
   "metadata": {},
   "source": [
    "# Task4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d087b68",
   "metadata": {},
   "source": [
    "1. Input Layer\n",
    "\n",
    "One-hot encoded word vector\n",
    "\n",
    "2. Hidden Layer\n",
    "\n",
    "Weights form the word embeddings\n",
    "\n",
    "3. Output Layer\n",
    "\n",
    "Softmax predicts probability of context words\n",
    "\n",
    "4. How weights become embeddings?\n",
    "\n",
    "The trained weight matrix between input and hidden layer becomes the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d93908",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Input → [ W1 ] → Embedding → [ W2 ] → Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1206c",
   "metadata": {},
   "source": [
    "# Part3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09344828",
   "metadata": {},
   "source": [
    "# Task5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3988e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"this\", \"movie\", \"is\", \"good\"],\n",
    "    [\"i\", \"liked\", \"this\", \"film\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46c49d",
   "metadata": {},
   "source": [
    "# Task6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f886d81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_cbow = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=0\n",
    ")\n",
    "\n",
    "print(\"Vocab size:\", len(model_cbow.wv))\n",
    "print(\"Vector for 'movie':\", model_cbow.wv[\"movie\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99202af",
   "metadata": {},
   "source": [
    "# Task7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577487ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_sg = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184cc52",
   "metadata": {},
   "source": [
    "# Task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07ab68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_sg.wv.most_similar(\"movie\")\n",
    "model_sg.wv.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3eaeb",
   "metadata": {},
   "source": [
    "# Part4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1545563",
   "metadata": {},
   "source": [
    "# Task9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54582fce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Task 9: Visualization\n",
    "\n",
    "Use PCA / TSNE to reduce to 2D and plot clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9801f2",
   "metadata": {},
   "source": [
    "# Task10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec248156",
   "metadata": {},
   "source": [
    "CBOW vs Skip-Gram\n",
    "CBOW is fast, Skip-Gram is accurate for rare words.\n",
    "\n",
    "Advantages over TF-IDF\n",
    "Captures semantics, lower dimensional, continuous.\n",
    "\n",
    "Limitations\n",
    "Cannot handle polysemy, static embeddings.\n",
    "\n",
    "Why context matters?\n",
    "Modern models like BERT use dynamic context."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
