{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800195a0",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf9466",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a261c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"text_data.csv\")   # your dataset\n",
    "df[\"text\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c11f54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# First 5 samples\n",
    "print(df[\"text\"].head())\n",
    "\n",
    "# Length of each text\n",
    "df[\"text_length\"] = df[\"text\"].astype(str).apply(len)\n",
    "df[[\"text\", \"text_length\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574ab22",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d4afb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)     # remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)         # remove numbers\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text_basic\"] = df[\"text\"].astype(str).apply(basic_clean)\n",
    "\n",
    "df[[\"text\", \"clean_text_basic\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51ae39",
   "metadata": {},
   "source": [
    "# Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbcebb",
   "metadata": {},
   "source": [
    "# Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0908b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def advanced_clean(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)       # URLs\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)       # Emails\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)         # HTML tags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)       # special chars\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text_advanced\"] = df[\"clean_text_basic\"].apply(advanced_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12042a9",
   "metadata": {},
   "source": [
    "# Task4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8de59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "df[\"text_no_stopwords\"] = df[\"clean_text_advanced\"].apply(\n",
    "    lambda x: \" \".join([w for w in x.split() if w not in stop_words])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e4c56",
   "metadata": {},
   "source": [
    "# Task5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2e494",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_repeats(text):\n",
    "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "slang = {\"u\":\"you\", \"gr8\":\"great\", \"pls\":\"please\"}\n",
    "\n",
    "def replace_slang(text):\n",
    "    return \" \".join([slang.get(w, w) for w in text.split()])\n",
    "\n",
    "df[\"text_normalized\"] = df[\"text_no_stopwords\"].apply(normalize_repeats).apply(replace_slang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac88596",
   "metadata": {},
   "source": [
    "# Part3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2926b698",
   "metadata": {},
   "source": [
    "# Task6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da07394",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "df[\"word_tokens\"] = df[\"text_normalized\"].apply(word_tokenize)\n",
    "df[\"sentence_tokens\"] = df[\"text\"].apply(sent_tokenize)\n",
    "\n",
    "df[[\"word_tokens\", \"sentence_tokens\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ae73c",
   "metadata": {},
   "source": [
    "# Task7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6e682",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "df[\"stemmed\"] = df[\"word_tokens\"].apply(lambda x: [ps.stem(w) for w in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20897d5f",
   "metadata": {},
   "source": [
    "# Task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e00195",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "df[\"lemmatized\"] = df[\"word_tokens\"].apply(lambda x: [lem.lemmatize(w) for w in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a441f0",
   "metadata": {},
   "source": [
    "# Task9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df102f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def nlp_preprocess(text):\n",
    "    text = basic_clean(text)\n",
    "    text = advanced_clean(text)\n",
    "    words = [w for w in text.split() if w not in stop_words]\n",
    "    tokens = [lem.lemmatize(w) for w in words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"final_clean_text\"] = df[\"text\"].apply(nlp_preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71792299",
   "metadata": {},
   "source": [
    "# Task10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc5de2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Basic cleaning removes visible noise.\n",
    "\n",
    "Advanced cleaning removes web & hidden noise.\n",
    "\n",
    "Lemmatization keeps real words (better than stemming).\n",
    "\n",
    "Preprocessing improves ML accuracy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
